{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Outlook      14 non-null     object\n",
      " 1   Temperature  14 non-null     object\n",
      " 2   Humidity     14 non-null     object\n",
      " 3   Wind         14 non-null     object\n",
      " 4   Play         14 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 688.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/assets/tennis.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total entropy of the dataset\n",
    "def calc_total_entropy(data, label, class_list):\n",
    "    total_row = data.shape[0]\n",
    "    total_entr = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        total_class_count = data[data[label] == c].shape[0] \n",
    "        total_class_entr = - (total_class_count/total_row)*np.log2(total_class_count/total_row) \n",
    "        total_entr += total_class_entr \n",
    "    \n",
    "    return total_entr\n",
    "\n",
    "#Calculating individual feature entropy\n",
    "def calc_feature_entropy(feature_info, label, class_list):\n",
    "    class_count = feature_info.shape[0]\n",
    "    entropy = 0\n",
    "    \n",
    "    for c in class_list:\n",
    "        label_class_count = feature_info[feature_info[label] == c].shape[0]\n",
    "        entropy_class = 0\n",
    "        if label_class_count != 0:\n",
    "            probability_class = label_class_count/class_count\n",
    "            entropy_class = - probability_class * np.log2(probability_class)  #entropy\n",
    "        entropy += entropy_class\n",
    "    return entropy\n",
    "\n",
    "#Calculating information gain\n",
    "def calc_info_gain(feature_name, train_data, label, class_list):\n",
    "    feature_value_list = train_data[feature_name].unique() \n",
    "    total_row = train_data.shape[0]\n",
    "    feature_info = 0.0\n",
    "    \n",
    "    for feature_value in feature_value_list:\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value] \n",
    "        feature_value_count = feature_value_data.shape[0]\n",
    "        feature_value_entropy = calc_feature_entropy(feature_value_data, label, class_list) \n",
    "        feature_value_probability = feature_value_count/total_row\n",
    "        feature_info += feature_value_probability * feature_value_entropy \n",
    "        \n",
    "    return calc_total_entropy(train_data, label, class_list) - feature_info \n",
    "\n",
    "#Finding correct node\n",
    "def find_most_informative_feature(train_data, label, class_list):\n",
    "    feature_list = train_data.columns.drop(label) \n",
    "    max_info_gain = -1\n",
    "    max_info_feature = None\n",
    "    \n",
    "    for feature in feature_list: \n",
    "        feature_info_gain = calc_info_gain(feature, train_data, label, class_list)\n",
    "        if max_info_gain < feature_info_gain: \n",
    "            max_info_gain = feature_info_gain\n",
    "            max_info_feature = feature\n",
    "            \n",
    "    return max_info_feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the tree\n",
    "def generate_sub_tree(feature_name, train_data, label, class_list):\n",
    "    feature_value_count_dict = train_data[feature_name].value_counts(sort=False)\n",
    "    tree = {} \n",
    "    \n",
    "    for feature_value, count in feature_value_count_dict.iteritems():\n",
    "        feature_value_data = train_data[train_data[feature_name] == feature_value] \n",
    "        \n",
    "        assigned_to_node = False\n",
    "        for c in class_list: \n",
    "            class_count = feature_value_data[feature_value_data[label] == c].shape[0] \n",
    "            if class_count == count: \n",
    "                tree[feature_value] = c \n",
    "                train_data = train_data[train_data[feature_name] != feature_value] \n",
    "                assigned_to_node = True\n",
    "        if not assigned_to_node:\n",
    "            tree[feature_value] = \"?\" \n",
    "            \n",
    "    return tree, train_data\n",
    "\n",
    "# ID3\n",
    "def make_tree(root, prev_feature_value, train_data, label, class_list):\n",
    "    if train_data.shape[0] != 0: \n",
    "        max_info_feature = find_most_informative_feature(train_data, label, class_list) \n",
    "        tree, train_data = generate_sub_tree(max_info_feature, train_data, label, class_list) \n",
    "        next_root = None\n",
    "        \n",
    "        if prev_feature_value != None: \n",
    "            root[prev_feature_value] = dict()\n",
    "            root[prev_feature_value][max_info_feature] = tree\n",
    "            next_root = root[prev_feature_value][max_info_feature]\n",
    "        else: \n",
    "            root[max_info_feature] = tree\n",
    "            next_root = root[max_info_feature]\n",
    "        \n",
    "        for node, branch in list(next_root.items()): \n",
    "            if branch == \"?\": \n",
    "                feature_value_data = train_data[train_data[max_info_feature] == node] \n",
    "                make_tree(next_root, node, feature_value_data, label, class_list)\n",
    "\n",
    "def id3(dataset, label):\n",
    "    data = dataset.copy() \n",
    "    tree = {}\n",
    "    class_list = data[label].unique()\n",
    "    make_tree(tree, None, data, label, class_list)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Outlook\": {\n",
      "    \"Sunny\": {\n",
      "      \"Humidity\": {\n",
      "        \"High\": \"No\",\n",
      "        \"Normal\": \"Yes\"\n",
      "      }\n",
      "    },\n",
      "    \"Overcast\": \"Yes\",\n",
      "    \"Rain\": {\n",
      "      \"Wind\": {\n",
      "        \"Weak\": \"Yes\",\n",
      "        \"Strong\": \"No\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "\n",
    "tree = id3(df, 'Play')\n",
    "print(json.dumps(tree, indent = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(string):\n",
    "    for i in range(0, len(string)):\n",
    "        if pd.isnull(string[i]) == False:          \n",
    "            try:\n",
    "                float(string[i])\n",
    "                return True\n",
    "            except ValueError:\n",
    "                return False\n",
    "            \n",
    "def is_number_value(value):\n",
    "    if pd.isnull(value) == False:          \n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "        \n",
    "def chi_squared_test(label_df, feature_df):\n",
    "    label_df.reset_index(drop=True, inplace=True)\n",
    "    feature_df.reset_index(drop=True, inplace=True)\n",
    "    data = pd.concat([pd.DataFrame(label_df.values.reshape((label_df.shape[0], 1))), feature_df], axis = 1)\n",
    "    data.columns=[\"label\", \"feature\"]\n",
    "    contigency_table = pd.crosstab(data.iloc[:,0], data.iloc[:,1], margins = False)\n",
    "    m = contigency_table.values.sum()\n",
    "    if m <= 10000 and contigency_table.shape == (2,2):\n",
    "        p_value = stats.fisher_exact(contigency_table)\n",
    "    else:\n",
    "        p_value = stats.chi2_contingency(contigency_table, correction = False) # (No Yates' Correction)\n",
    "    return p_value[1]\n",
    "\n",
    "def prediction_dt_c45(model, Xdata):\n",
    "    Xdata = Xdata.reset_index(drop=True)\n",
    "    ydata = pd.DataFrame(index=range(0, Xdata.shape[0]), columns=[\"Prediction\"])\n",
    "    data  = pd.concat([ydata, Xdata], axis = 1)\n",
    "    rule = []\n",
    "    \n",
    "    for j in range(0, data.shape[1]):\n",
    "        if data.iloc[:,j].dtype == \"bool\":\n",
    "            data.iloc[:,j] = data.iloc[:, j].astype(str)\n",
    "                   \n",
    "    dt_model = deepcopy(model)\n",
    "    \n",
    "    for i in range(0, len(dt_model)):\n",
    "        dt_model[i] = dt_model[i].replace(\"{\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"}\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\";\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"IF \", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"AND\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"THEN\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"=\", \"\")\n",
    "        dt_model[i] = dt_model[i].replace(\"<\", \"<=\")\n",
    "    \n",
    "    for i in range(0, len(dt_model) -2): \n",
    "        splited_rule = [x for x in dt_model[i].split(\" \") if x]\n",
    "        rule.append(splited_rule)\n",
    "   \n",
    "    for i in range(0, Xdata.shape[0]): \n",
    "        for j in range(0, len(rule)):\n",
    "            rule_confirmation = len(rule[j])/2 - 1\n",
    "            rule_count = 0\n",
    "            for k in range(0, len(rule[j]) - 2, 2):\n",
    "                if is_number_value(data[rule[j][k]][i]) == False:\n",
    "                    if (data[rule[j][k]][i] in rule[j][k+1]):\n",
    "                        rule_count = rule_count + 1\n",
    "                        if (rule_count == rule_confirmation):\n",
    "                            data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                    else:\n",
    "                        k = len(rule[j])\n",
    "                elif is_number_value(data[rule[j][k]][i]) == True:\n",
    "                     if rule[j][k+1].find(\"<=\") == 0:\n",
    "                         if data[rule[j][k]][i] <= float(rule[j][k+1].replace(\"<=\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "                     elif rule[j][k+1].find(\">\") == 0:\n",
    "                         if data[rule[j][k]][i] > float(rule[j][k+1].replace(\">\", \"\")): \n",
    "                             rule_count = rule_count + 1\n",
    "                             if (rule_count == rule_confirmation):\n",
    "                                 data.iloc[i,0] = rule[j][len(rule[j]) - 1]\n",
    "                         else:\n",
    "                             k = len(rule[j])\n",
    "    \n",
    "    for i in range(0, Xdata.shape[0]):\n",
    "        if pd.isnull(data.iloc[i,0]):\n",
    "            data.iloc[i,0] = dt_model[len(dt_model)-1]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_gain_ratio(target, feature = [], uniques = []):\n",
    "    entropy = 0\n",
    "    denominator_1 = feature.count()\n",
    "    data = pd.concat([pd.DataFrame(target.values.reshape((target.shape[0], 1))), feature], axis = 1)\n",
    "    for entp in range(0, len(np.unique(target))):\n",
    "        numerator_1 = data.iloc[:,0][(data.iloc[:,0] == np.unique(target)[entp])].count()\n",
    "        if numerator_1 > 0:\n",
    "            entropy = entropy - (numerator_1/denominator_1)* np.log2((numerator_1/denominator_1))\n",
    "    info_gain = float(entropy)\n",
    "    info_gain_r = 0\n",
    "    intrinsic_v = 0\n",
    "    for word in range(0, len(uniques)):\n",
    "        denominator_2 = feature[(feature == uniques[word])].count()\n",
    "        if denominator_2[0] > 0:\n",
    "            intrinsic_v = intrinsic_v - (denominator_2/denominator_1)* np.log2((denominator_2/denominator_1))\n",
    "        for lbl in range(0, len(np.unique(target))):\n",
    "            numerator_2 = data.iloc[:,0][(data.iloc[:,0] == np.unique(target)[lbl]) & (data.iloc[:,1]  == uniques[word])].count()\n",
    "            if numerator_2 > 0:\n",
    "                info_gain = info_gain + (denominator_2/denominator_1)*(numerator_2/denominator_2)* np.log2((numerator_2/denominator_2))\n",
    "    if intrinsic_v[0] > 0:\n",
    "        info_gain_r = info_gain/intrinsic_v\n",
    "    return float(info_gain_r)\n",
    "\n",
    "def split_me(feature, split):\n",
    "    result = pd.DataFrame(feature.values.reshape((feature.shape[0], 1)))\n",
    "    for fill in range(0, len(feature)):\n",
    "        result.iloc[fill,0] = feature.iloc[fill]\n",
    "    lower = \"<=\" + str(split)\n",
    "    upper = \">\" + str(split)\n",
    "    for convert in range(0, len(feature)):\n",
    "        if float(feature.iloc[convert]) <= float(split):\n",
    "            result.iloc[convert,0] = lower\n",
    "        else:\n",
    "            result.iloc[convert,0] = upper\n",
    "    binary_split = []\n",
    "    binary_split = [lower, upper]\n",
    "    return result, binary_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_c45(Xdata, ydata, cat_missing = \"none\", num_missing = \"none\", pre_pruning = \"none\", chi_lim = 0.1, min_lim = 5):\n",
    "    ydata = ydata.apply(str)\n",
    "    name  = ydata.name\n",
    "    if (name is None):\n",
    "        name  = 'class'\n",
    "    ydata = pd.DataFrame(ydata.values.reshape((ydata.shape[0], 1)))\n",
    "    for j in range(0, ydata.shape[1]):\n",
    "        if ydata.iloc[:,j].dropna().value_counts().index.isin([0,1]).all():\n",
    "            for i in range(0, ydata.shape[0]):          \n",
    "               if ydata.iloc[i,j] == 0:\n",
    "                   ydata.iloc[i,j] = \"zero\"\n",
    "               else:\n",
    "                   ydata.iloc[i,j] = \"one\"\n",
    "    dataset = pd.concat([ydata, Xdata], axis = 1)\n",
    "    \n",
    "    for j in range(0, dataset.shape[1]):\n",
    "        if dataset.iloc[:,j].dtype == \"bool\":\n",
    "            dataset.iloc[:,j] = dataset.iloc[:, j].astype(str)\n",
    "\n",
    "    if cat_missing != \"none\":\n",
    "        for j in range(1, dataset.shape[1]): \n",
    "            if is_number(dataset.iloc[:, j]) == False:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if cat_missing == \"missing\":\n",
    "                            dataset.iloc[i,j] = \"Unknow\"\n",
    "                        elif cat_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif cat_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]            \n",
    "    elif num_missing != \"none\":\n",
    "            if is_number(dataset.iloc[:, j]) == True:\n",
    "                for i in range(0, dataset.shape[0]):\n",
    "                    if pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                        if num_missing == \"mean\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].mean()\n",
    "                        elif num_missing == \"median\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].median()\n",
    "                        elif num_missing == \"most\":\n",
    "                            dataset.iloc[i,j] = dataset.iloc[:,j].value_counts().idxmax()\n",
    "                        elif cat_missing == \"remove\":\n",
    "                            dataset = dataset.drop(dataset.index[i], axis = 0)\n",
    "                        elif num_missing == \"probability\":\n",
    "                            while pd.isnull(dataset.iloc[i,j]) == True:\n",
    "                                dataset.iloc[i,j] = dataset.iloc[randint(0, dataset.shape[0] - 1), j]  \n",
    "    \n",
    "    unique = []\n",
    "    uniqueWords = []\n",
    "    for j in range(0, dataset.shape[1]): \n",
    "        for i in range(0, dataset.shape[0]):\n",
    "            token = dataset.iloc[i, j]\n",
    "            if not token in unique:\n",
    "                unique.append(token)\n",
    "        uniqueWords.append(unique)\n",
    "        unique = []  \n",
    "    \n",
    "    label = np.array(uniqueWords[0])\n",
    "    label = label.reshape(1, len(uniqueWords[0]))\n",
    "    \n",
    "    i = 0\n",
    "    impurity = 0\n",
    "    branch = [None]*1\n",
    "    branch[0] = dataset\n",
    "    gain_ratio = np.empty([1, branch[i].shape[1]])\n",
    "    lower = \"0\"\n",
    "    root_index = 0\n",
    "    rule = [None]*1\n",
    "    rule[0] = \"IF \"\n",
    "    skip_update = False\n",
    "    stop = 2\n",
    "    upper = \"1\"\n",
    "    \n",
    "    while (i < stop):\n",
    "        impurity = np.amax(gain_ratio)\n",
    "        gain_ratio.fill(0)\n",
    "        for element in range(1, branch[i].shape[1]):\n",
    "            if len(branch[i]) == 0:\n",
    "                skip_update = True \n",
    "                break\n",
    "            if len(np.unique(branch[i][0])) == 1 or len(branch[i]) == 1:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].iloc[0, 0] + \";\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 break\n",
    "            if i > 0 and is_number(dataset.iloc[:, element]) == False and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], branch[i].iloc[:, element]) > chi_lim:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "            if is_number(dataset.iloc[:, element]) == True:\n",
    "                gain_ratio[0, element] = 0.0\n",
    "                value = np.sort(branch[i].iloc[:, element].unique())\n",
    "                skip_update = False\n",
    "                if branch[i][(branch[i].iloc[:, element] == value[0])].count()[0] > 1:\n",
    "                    start = 0\n",
    "                    finish = len(branch[i].iloc[:, element].unique()) - 2\n",
    "                else:\n",
    "                    start = 1\n",
    "                    finish = len(branch[i].iloc[:, element].unique()) - 2\n",
    "                if len(branch[i]) == 2 or len(value) == 1 or len(value) == 2:\n",
    "                    start = 0\n",
    "                    finish = 1\n",
    "                if len(value) == 3:\n",
    "                    start = 0\n",
    "                    finish = 2\n",
    "                for bin_split in range(start, finish):\n",
    "                    bin_sample = split_me(feature = branch[i].iloc[:, element], split = value[bin_split])\n",
    "                    if i > 0 and pre_pruning == \"chi_2\" and chi_squared_test(branch[i].iloc[:, 0], bin_sample[0]) > chi_lim:\n",
    "                        if \";\" not in rule[i]:\n",
    "                             rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                             rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                        skip_update = True\n",
    "                        continue\n",
    "                    igr = info_gain_ratio(target = branch[i].iloc[:, 0], feature = bin_sample[0], uniques = bin_sample[1])\n",
    "                    if igr > float(gain_ratio[0, element]):\n",
    "                        gain_ratio[0, element] = igr\n",
    "                        uniqueWords[element] = bin_sample[1]\n",
    "            if is_number(dataset.iloc[:, element]) == False:\n",
    "                gain_ratio[0, element] = 0.0\n",
    "                skip_update = False\n",
    "                igr = info_gain_ratio(target = branch[i].iloc[:, 0], feature =  pd.DataFrame(branch[i].iloc[:, element].values.reshape((branch[i].iloc[:, element].shape[0], 1))), uniques = uniqueWords[element])\n",
    "                gain_ratio[0, element] = igr\n",
    "            if i > 0 and pre_pruning == \"min\" and len(branch[i]) <= min_lim:\n",
    "                 if \";\" not in rule[i]:\n",
    "                     rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                     rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "                 skip_update = True\n",
    "                 continue\n",
    "           \n",
    "        if i > 0 and pre_pruning == \"impur\" and np.amax(gain_ratio) < impurity and np.amax(gain_ratio) > 0:\n",
    "             if \";\" not in rule[i]:\n",
    "                 rule[i] = rule[i] + \" THEN \" + name + \" = \" + branch[i].agg(lambda x:x.value_counts().index[0])[0] + \";\"\n",
    "                 rule[i] = rule[i].replace(\" AND  THEN \", \" THEN \")\n",
    "             skip_update = True\n",
    "             continue\n",
    "        \n",
    "        if skip_update == False:\n",
    "            root_index = np.argmax(gain_ratio)\n",
    "            rule[i] = rule[i] + str(list(branch[i])[root_index])\n",
    "            \n",
    "            for word in range(0, len(uniqueWords[root_index])):\n",
    "                uw = uniqueWords[root_index][word].replace(\"<=\", \"\")\n",
    "                uw = uw.replace(\">\", \"\")\n",
    "                lower = \"<=\" + uw\n",
    "                upper = \">\" + uw\n",
    "                if uniqueWords[root_index][word] == lower:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] <= float(uw)])\n",
    "                elif uniqueWords[root_index][word] == upper:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index]  > float(uw)])\n",
    "                else:\n",
    "                    branch.append(branch[i][branch[i].iloc[:, root_index] == uniqueWords[root_index][word]])\n",
    "                \n",
    "                rule.append(rule[i] + \" = \" + \"{\" + uniqueWords[root_index][word] + \"}\")\n",
    "            \n",
    "            for logic_connection in range(1, len(rule)):\n",
    "                if len(np.unique(branch[i][0])) != 1 and rule[logic_connection].endswith(\" AND \") == False  and rule[logic_connection].endswith(\"}\") == True:\n",
    "                    rule[logic_connection] = rule[logic_connection] + \" AND \"\n",
    "        skip_update = False\n",
    "        i = i + 1\n",
    "        stop = len(rule)\n",
    "    \n",
    "    for i in range(len(rule) - 1, -1, -1):\n",
    "        if rule[i].endswith(\";\") == False:\n",
    "            del rule[i]    \n",
    "\n",
    "    rule.append(\"Total Number of Rules: \" + str(len(rule)))\n",
    "    rule.append(dataset.agg(lambda x:x.value_counts().index[0])[0])\n",
    "    \n",
    "    return rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Prediction Outlook Temperature Humidity    Wind\n",
       "0         No   Sunny         Hot     High    Weak\n",
       "1         No   Sunny         Hot     High  Strong"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/assets/tennis.csv')\n",
    "\n",
    "X = df.iloc[:, 0:4]\n",
    "y = df.iloc[:, 4]\n",
    "\n",
    "dt_model = dt_c45(Xdata = X, ydata = y, cat_missing = \"missing\", num_missing = \"mean\", pre_pruning = \"impur\", chi_lim = 0.1, min_lim = 5)\n",
    "\n",
    "test =  df.iloc[0:2, 0:4]\n",
    "prediction_dt_c45(dt_model, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Outlook Temperature Humidity    Wind Play\n",
       "0     Sunny         Hot     High    Weak   No\n",
       "1     Sunny         Hot     High  Strong   No\n",
       "2  Overcast         Hot     High    Weak  Yes\n",
       "3      Rain        Mild     High    Weak  Yes\n",
       "4      Rain        Cool   Normal    Weak  Yes"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn import metrics \n",
    "\n",
    "df = pd.read_csv(\"/assets/tennis.csv\") \n",
    "\n",
    "# Conversion Table\n",
    "conversions = {\n",
    "    \"Outlook\" : {\n",
    "        \"Sunny\" : 0,\n",
    "        \"Overcast\" : 1,\n",
    "        \"Rain\" : 2\n",
    "    },\n",
    "    \"Temperature\" : {\n",
    "        \"Hot\" : 0,\n",
    "        \"Mild\" : 1,\n",
    "        \"Cool\" : 2\n",
    "    },\n",
    "    \"Humidity\" : {\n",
    "        \"High\" : 0,\n",
    "        \"Low\" : 1\n",
    "    },\n",
    "    \"Wind\" : {\n",
    "        \"Weak\" : 0,\n",
    "        \"Strong\" : 1\n",
    "    },\n",
    "    \"Play\" : {\n",
    "        \"No\" : 0,\n",
    "        \"Yes\" : 1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:4]\n",
    "y = df.iloc[:, 4]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=2)\n",
    "clf = clf.fit(X_train,y_train) \n",
    "y_pred = clf.predict(X_test) \n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
